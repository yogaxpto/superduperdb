"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[4767],{1195:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>c,toc:()=>l});var s=n(4848),d=n(8453);const r={},i=void 0,c={id:"api/ext/openai/model",title:"model",description:"superduperdb.ext.openai.model",source:"@site/content/api/ext/openai/model.md",sourceDirName:"api/ext/openai",slug:"/api/ext/openai/model",permalink:"/docs/api/ext/openai/model",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/api/ext/openai/model.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"encoder",permalink:"/docs/api/ext/numpy/encoder"},next:{title:"encoder",permalink:"/docs/api/ext/pillow/encoder"}},a={},l=[{value:"<code>OpenAIChatCompletion</code>",id:"openaichatcompletion",level:2},{value:"<code>OpenAIEmbedding</code>",id:"openaiembedding",level:2},{value:"<code>OpenAIAudioTranscription</code>",id:"openaiaudiotranscription",level:2},{value:"<code>OpenAIAudioTranslation</code>",id:"openaiaudiotranslation",level:2},{value:"<code>OpenAIImageCreation</code>",id:"openaiimagecreation",level:2},{value:"<code>OpenAIImageEdit</code>",id:"openaiimageedit",level:2}];function o(e){const t={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,d.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.code,{children:"superduperdb.ext.openai.model"})})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://github.com/SuperDuperDB/superduperdb/blob/main/superduperdb/ext/openai/model.py",children:"Source code"})}),"\n",(0,s.jsx)(t.h2,{id:"openaichatcompletion",children:(0,s.jsx)(t.code,{children:"OpenAIChatCompletion"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"OpenAIChatCompletion(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = <factory>,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = <factory>,\n     predict_kwargs: 't.Dict' = <factory>,\n     compute_kwargs: 't.Dict' = <factory>,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = <factory>,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = <factory>,\n     batch_size: int = 1,\n     prompt: str = '') -> None\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"identifier"}),(0,s.jsx)(t.td,{children:"Identifier of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"db"}),(0,s.jsx)(t.td,{children:"Datalayer instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"uuid"}),(0,s.jsx)(t.td,{children:"UUID of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"artifacts"}),(0,s.jsxs)(t.td,{children:["A dictionary of artifacts paths and ",(0,s.jsx)(t.code,{children:"DataType"})," objects"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"signature"}),(0,s.jsx)(t.td,{children:"Model signature."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"datatype"}),(0,s.jsx)(t.td,{children:"DataType instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"output_schema"}),(0,s.jsx)(t.td,{children:"Output schema (mapping of encoders)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"flatten"}),(0,s.jsx)(t.td,{children:"Flatten the model outputs."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model_update_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to use for model update."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"predict_kwargs"}),(0,s.jsx)(t.td,{children:"Additional arguments to use at prediction time."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"compute_kwargs"}),(0,s.jsx)(t.td,{children:"Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"validation"}),(0,s.jsxs)(t.td,{children:["The validation ",(0,s.jsx)(t.code,{children:"Dataset"})," instances to use."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"metric_values"}),(0,s.jsx)(t.td,{children:"The metrics to evaluate on."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model"}),(0,s.jsxs)(t.td,{children:["The Model to use, e.g. ",(0,s.jsx)(t.code,{children:"'text-embedding-ada-002'"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"max_batch_size"}),(0,s.jsx)(t.td,{children:"Maximum  batch size."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_key"}),(0,s.jsx)(t.td,{children:"The OpenAI API key."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_base"}),(0,s.jsx)(t.td,{children:"The server to use for requests."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"client_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to be passed to OpenAI"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"batch_size"}),(0,s.jsx)(t.td,{children:"The batch size to use."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"prompt"}),(0,s.jsx)(t.td,{children:"The prompt to use to seed the response."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"OpenAI chat completion predictor."}),"\n",(0,s.jsx)(t.h2,{id:"openaiembedding",children:(0,s.jsx)(t.code,{children:"OpenAIEmbedding"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"OpenAIEmbedding(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = <factory>,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = <factory>,\n     predict_kwargs: 't.Dict' = <factory>,\n     compute_kwargs: 't.Dict' = <factory>,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = <factory>,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = <factory>,\n     shape: Optional[Sequence[int]] = None,\n     batch_size: int = 100) -> None\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"identifier"}),(0,s.jsx)(t.td,{children:"Identifier of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"db"}),(0,s.jsx)(t.td,{children:"Datalayer instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"uuid"}),(0,s.jsx)(t.td,{children:"UUID of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"artifacts"}),(0,s.jsxs)(t.td,{children:["A dictionary of artifacts paths and ",(0,s.jsx)(t.code,{children:"DataType"})," objects"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"signature"}),(0,s.jsx)(t.td,{children:"Model signature."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"datatype"}),(0,s.jsx)(t.td,{children:"DataType instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"output_schema"}),(0,s.jsx)(t.td,{children:"Output schema (mapping of encoders)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"flatten"}),(0,s.jsx)(t.td,{children:"Flatten the model outputs."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model_update_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to use for model update."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"predict_kwargs"}),(0,s.jsx)(t.td,{children:"Additional arguments to use at prediction time."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"compute_kwargs"}),(0,s.jsx)(t.td,{children:"Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"validation"}),(0,s.jsxs)(t.td,{children:["The validation ",(0,s.jsx)(t.code,{children:"Dataset"})," instances to use."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"metric_values"}),(0,s.jsx)(t.td,{children:"The metrics to evaluate on."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model"}),(0,s.jsxs)(t.td,{children:["The Model to use, e.g. ",(0,s.jsx)(t.code,{children:"'text-embedding-ada-002'"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"max_batch_size"}),(0,s.jsx)(t.td,{children:"Maximum  batch size."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_key"}),(0,s.jsx)(t.td,{children:"The OpenAI API key."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_base"}),(0,s.jsx)(t.td,{children:"The server to use for requests."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"client_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to be passed to OpenAI"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"shape"}),(0,s.jsxs)(t.td,{children:["The shape as ",(0,s.jsx)(t.code,{children:"tuple"})," of the embedding."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"batch_size"}),(0,s.jsx)(t.td,{children:"The batch size to use."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"OpenAI embedding predictor."}),"\n",(0,s.jsx)(t.h2,{id:"openaiaudiotranscription",children:(0,s.jsx)(t.code,{children:"OpenAIAudioTranscription"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"OpenAIAudioTranscription(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = <factory>,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = <factory>,\n     predict_kwargs: 't.Dict' = <factory>,\n     compute_kwargs: 't.Dict' = <factory>,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = <factory>,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = <factory>,\n     takes_context: bool = True,\n     prompt: str = '') -> None\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"identifier"}),(0,s.jsx)(t.td,{children:"Identifier of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"db"}),(0,s.jsx)(t.td,{children:"Datalayer instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"uuid"}),(0,s.jsx)(t.td,{children:"UUID of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"artifacts"}),(0,s.jsxs)(t.td,{children:["A dictionary of artifacts paths and ",(0,s.jsx)(t.code,{children:"DataType"})," objects"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"signature"}),(0,s.jsx)(t.td,{children:"Model signature."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"datatype"}),(0,s.jsx)(t.td,{children:"DataType instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"output_schema"}),(0,s.jsx)(t.td,{children:"Output schema (mapping of encoders)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"flatten"}),(0,s.jsx)(t.td,{children:"Flatten the model outputs."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model_update_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to use for model update."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"predict_kwargs"}),(0,s.jsx)(t.td,{children:"Additional arguments to use at prediction time."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"compute_kwargs"}),(0,s.jsx)(t.td,{children:"Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"validation"}),(0,s.jsxs)(t.td,{children:["The validation ",(0,s.jsx)(t.code,{children:"Dataset"})," instances to use."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"metric_values"}),(0,s.jsx)(t.td,{children:"The metrics to evaluate on."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model"}),(0,s.jsxs)(t.td,{children:["The Model to use, e.g. ",(0,s.jsx)(t.code,{children:"'text-embedding-ada-002'"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"max_batch_size"}),(0,s.jsx)(t.td,{children:"Maximum  batch size."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_key"}),(0,s.jsx)(t.td,{children:"The OpenAI API key."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_base"}),(0,s.jsx)(t.td,{children:"The server to use for requests."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"client_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to be passed to OpenAI"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"takes_context"}),(0,s.jsx)(t.td,{children:"Whether the model takes context into account."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"prompt"}),(0,s.jsx)(t.td,{children:"The prompt to guide the model's style."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"OpenAI audio transcription predictor."}),"\n",(0,s.jsxs)(t.p,{children:["The prompt should contain the ",(0,s.jsx)(t.code,{children:'"context"'})," format variable."]}),"\n",(0,s.jsx)(t.h2,{id:"openaiaudiotranslation",children:(0,s.jsx)(t.code,{children:"OpenAIAudioTranslation"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"OpenAIAudioTranslation(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = <factory>,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = <factory>,\n     predict_kwargs: 't.Dict' = <factory>,\n     compute_kwargs: 't.Dict' = <factory>,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = <factory>,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = <factory>,\n     takes_context: bool = True,\n     prompt: str = '',\n     batch_size: int = 1) -> None\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"identifier"}),(0,s.jsx)(t.td,{children:"Identifier of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"db"}),(0,s.jsx)(t.td,{children:"Datalayer instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"uuid"}),(0,s.jsx)(t.td,{children:"UUID of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"artifacts"}),(0,s.jsxs)(t.td,{children:["A dictionary of artifacts paths and ",(0,s.jsx)(t.code,{children:"DataType"})," objects"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"signature"}),(0,s.jsx)(t.td,{children:"Model signature."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"datatype"}),(0,s.jsx)(t.td,{children:"DataType instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"output_schema"}),(0,s.jsx)(t.td,{children:"Output schema (mapping of encoders)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"flatten"}),(0,s.jsx)(t.td,{children:"Flatten the model outputs."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model_update_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to use for model update."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"predict_kwargs"}),(0,s.jsx)(t.td,{children:"Additional arguments to use at prediction time."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"compute_kwargs"}),(0,s.jsx)(t.td,{children:"Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"validation"}),(0,s.jsxs)(t.td,{children:["The validation ",(0,s.jsx)(t.code,{children:"Dataset"})," instances to use."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"metric_values"}),(0,s.jsx)(t.td,{children:"The metrics to evaluate on."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model"}),(0,s.jsxs)(t.td,{children:["The Model to use, e.g. ",(0,s.jsx)(t.code,{children:"'text-embedding-ada-002'"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"max_batch_size"}),(0,s.jsx)(t.td,{children:"Maximum  batch size."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_key"}),(0,s.jsx)(t.td,{children:"The OpenAI API key."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_base"}),(0,s.jsx)(t.td,{children:"The server to use for requests."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"client_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to be passed to OpenAI"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"takes_context"}),(0,s.jsx)(t.td,{children:"Whether the model takes context into account."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"prompt"}),(0,s.jsx)(t.td,{children:"The prompt to guide the model's style."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"batch_size"}),(0,s.jsx)(t.td,{children:"The batch size to use."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"OpenAI audio translation predictor."}),"\n",(0,s.jsxs)(t.p,{children:["The prompt should contain the ",(0,s.jsx)(t.code,{children:'"context"'})," format variable."]}),"\n",(0,s.jsx)(t.h2,{id:"openaiimagecreation",children:(0,s.jsx)(t.code,{children:"OpenAIImageCreation"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"OpenAIImageCreation(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = <factory>,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = <factory>,\n     predict_kwargs: 't.Dict' = <factory>,\n     compute_kwargs: 't.Dict' = <factory>,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = <factory>,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = <factory>,\n     takes_context: bool = True,\n     prompt: str = '',\n     n: int = 1,\n     response_format: str = 'b64_json') -> None\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"identifier"}),(0,s.jsx)(t.td,{children:"Identifier of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"db"}),(0,s.jsx)(t.td,{children:"Datalayer instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"uuid"}),(0,s.jsx)(t.td,{children:"UUID of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"artifacts"}),(0,s.jsxs)(t.td,{children:["A dictionary of artifacts paths and ",(0,s.jsx)(t.code,{children:"DataType"})," objects"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"signature"}),(0,s.jsx)(t.td,{children:"Model signature."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"datatype"}),(0,s.jsx)(t.td,{children:"DataType instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"output_schema"}),(0,s.jsx)(t.td,{children:"Output schema (mapping of encoders)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"flatten"}),(0,s.jsx)(t.td,{children:"Flatten the model outputs."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model_update_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to use for model update."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"predict_kwargs"}),(0,s.jsx)(t.td,{children:"Additional arguments to use at prediction time."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"compute_kwargs"}),(0,s.jsx)(t.td,{children:"Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"validation"}),(0,s.jsxs)(t.td,{children:["The validation ",(0,s.jsx)(t.code,{children:"Dataset"})," instances to use."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"metric_values"}),(0,s.jsx)(t.td,{children:"The metrics to evaluate on."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model"}),(0,s.jsxs)(t.td,{children:["The Model to use, e.g. ",(0,s.jsx)(t.code,{children:"'text-embedding-ada-002'"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"max_batch_size"}),(0,s.jsx)(t.td,{children:"Maximum  batch size."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_key"}),(0,s.jsx)(t.td,{children:"The OpenAI API key."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_base"}),(0,s.jsx)(t.td,{children:"The server to use for requests."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"client_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to be passed to OpenAI"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"takes_context"}),(0,s.jsx)(t.td,{children:"Whether the model takes context into account."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"prompt"}),(0,s.jsx)(t.td,{children:"The prompt to use to seed the response."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"n"}),(0,s.jsx)(t.td,{children:"The number of images to generate."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"response_format"}),(0,s.jsx)(t.td,{children:"The response format to use."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"OpenAI image creation predictor."}),"\n",(0,s.jsx)(t.h2,{id:"openaiimageedit",children:(0,s.jsx)(t.code,{children:"OpenAIImageEdit"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"OpenAIImageEdit(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = <factory>,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = <factory>,\n     predict_kwargs: 't.Dict' = <factory>,\n     compute_kwargs: 't.Dict' = <factory>,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = <factory>,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = <factory>,\n     takes_context: bool = True,\n     prompt: str = '',\n     response_format: str = 'b64_json',\n     n: int = 1) -> None\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Parameter"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"identifier"}),(0,s.jsx)(t.td,{children:"Identifier of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"db"}),(0,s.jsx)(t.td,{children:"Datalayer instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"uuid"}),(0,s.jsx)(t.td,{children:"UUID of the leaf."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"artifacts"}),(0,s.jsxs)(t.td,{children:["A dictionary of artifacts paths and ",(0,s.jsx)(t.code,{children:"DataType"})," objects"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"signature"}),(0,s.jsx)(t.td,{children:"Model signature."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"datatype"}),(0,s.jsx)(t.td,{children:"DataType instance."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"output_schema"}),(0,s.jsx)(t.td,{children:"Output schema (mapping of encoders)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"flatten"}),(0,s.jsx)(t.td,{children:"Flatten the model outputs."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model_update_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to use for model update."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"predict_kwargs"}),(0,s.jsx)(t.td,{children:"Additional arguments to use at prediction time."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"compute_kwargs"}),(0,s.jsx)(t.td,{children:"Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"validation"}),(0,s.jsxs)(t.td,{children:["The validation ",(0,s.jsx)(t.code,{children:"Dataset"})," instances to use."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"metric_values"}),(0,s.jsx)(t.td,{children:"The metrics to evaluate on."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"model"}),(0,s.jsxs)(t.td,{children:["The Model to use, e.g. ",(0,s.jsx)(t.code,{children:"'text-embedding-ada-002'"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"max_batch_size"}),(0,s.jsx)(t.td,{children:"Maximum  batch size."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_key"}),(0,s.jsx)(t.td,{children:"The OpenAI API key."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"openai_api_base"}),(0,s.jsx)(t.td,{children:"The server to use for requests."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"client_kwargs"}),(0,s.jsx)(t.td,{children:"The kwargs to be passed to OpenAI"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"takes_context"}),(0,s.jsx)(t.td,{children:"Whether the model takes context into account."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"prompt"}),(0,s.jsx)(t.td,{children:"The prompt to use to seed the response."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"response_format"}),(0,s.jsx)(t.td,{children:"The response format to use."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"n"}),(0,s.jsx)(t.td,{children:"The number of images to generate."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"OpenAI image edit predictor."})]})}function h(e={}){const{wrapper:t}={...(0,d.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>c});var s=n(6540);const d={},r=s.createContext(d);function i(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:i(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);