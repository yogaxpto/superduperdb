"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[5946],{6227:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>b,frontMatter:()=>i,metadata:()=>u,toc:()=>m});var r=n(4848),o=n(8453),a=n(9489),s=n(7227),l=n(94);const i={sidebar_label:"Build multimodal embedding models",filename:"build_multimodal_embedding_models.md"},d="Build multimodal embedding models",u={id:"reusable_snippets/build_multimodal_embedding_models",title:"build_multimodal_embedding_models",description:"Some embedding models such as CLIP come in pairs of model and compatible_model.",source:"@site/content/reusable_snippets/build_multimodal_embedding_models.md",sourceDirName:"reusable_snippets",slug:"/reusable_snippets/build_multimodal_embedding_models",permalink:"/docs/reusable_snippets/build_multimodal_embedding_models",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/reusable_snippets/build_multimodal_embedding_models.md",tags:[],version:"current",frontMatter:{sidebar_label:"Build multimodal embedding models",filename:"build_multimodal_embedding_models.md"},sidebar:"tutorialSidebar",previous:{title:"Build image embedding model",permalink:"/docs/reusable_snippets/build_image_embedding_model"},next:{title:"Build LLM",permalink:"/docs/reusable_snippets/build_llm"}},c={},m=[];function p(e){const t={a:"a",code:"code",h1:"h1",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h1,{id:"build-multimodal-embedding-models",children:"Build multimodal embedding models"}),"\n",(0,r.jsxs)(t.p,{children:["Some embedding models such as ",(0,r.jsx)(t.a,{href:"https://github.com/openai/CLIP",children:"CLIP"})," come in pairs of ",(0,r.jsx)(t.code,{children:"model"})," and ",(0,r.jsx)(t.code,{children:"compatible_model"}),".\nOtherwise:"]}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsx)(s.A,{value:"Text",label:"Text",default:!0,children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"from superduperdb.ext.sentence_transformers import SentenceTransformer\n\n# Load the pre-trained sentence transformer model\nmodel = SentenceTransformer(\n    identifier='all-MiniLM-L6-v2',\n    postprocess=lambda x: x.tolist(),\n)        \n"})})}),(0,r.jsx)(s.A,{value:"Image",label:"Image",default:!0,children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"from torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nimport warnings\n\n# Import custom modules\nfrom superduperdb.ext.torch import TorchModel, tensor\n\n# Define a series of image transformations using torchvision.transforms.Compose\nt = transforms.Compose([\n    transforms.Resize((224, 224)),   # Resize the input image to 224x224 pixels (must same as here)\n    transforms.CenterCrop((224, 224)),  # Perform a center crop on the resized image\n    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the tensor with specified mean and standard deviation\n])\n\n# Define a preprocess function that applies the defined transformations to an input image\ndef preprocess(x):\n    try:\n        return t(x)\n    except Exception as e:\n        # If an exception occurs during preprocessing, issue a warning and return a tensor of zeros\n        warnings.warn(str(e))\n        return torch.zeros(3, 224, 224)\n\n# Load the pre-trained ResNet-50 model from torchvision\nresnet50 = models.resnet50(pretrained=True)\n\n# Extract all layers of the ResNet-50 model except the last one\nmodules = list(resnet50.children())[:-1]\nresnet50 = nn.Sequential(*modules)\n\n# Create a TorchModel instance with the ResNet-50 model, preprocessing function, and postprocessing lambda\nmodel = TorchModel(\n    identifier='resnet50',\n    preprocess=preprocess,\n    object=resnet50,\n    postprocess=lambda x: x[:, 0, 0],  # Postprocess by extracting the top-left element of the output tensor\n    datatype=tensor(dtype='float', shape=(2048,))  # Specify the encoder configuration\n)        \n"})})}),(0,r.jsx)(s.A,{value:"Text-Image",label:"Text-Image",default:!0,children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"!pip install git+https://github.com/openai/CLIP.git\nimport clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\n# Load the CLIP model and obtain the preprocessing function\nmodel, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n\n# Define a vector with shape (1024,)\n\noutput_datatpye = vector(shape=(1024,))\n\n# Create a TorchModel for text encoding\ncompatible_model = TorchModel(\n    identifier='clip_text', # Unique identifier for the model\n    object=model, # CLIP model\n    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n)\n\n# Create a TorchModel for visual encoding\nmodel = TorchModel(\n    identifier='clip_image',  # Unique identifier for the model\n    object=model.visual,  # Visual part of the CLIP model    \n    preprocess=preprocess, # Visual preprocessing using CLIP\n    postprocess=lambda x: x.tolist(), # Convert the output to a list \n    datatype=output_datatpye, # Vector encoder with shape (1024,)\n)        \n"})})}),(0,r.jsx)(s.A,{value:"Audio",label:"Audio",default:!0,children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"!pip install librosa\nimport librosa\nimport numpy as np\nfrom superduperdb import ObjectModel\nfrom superduperdb import vector\n\ndef audio_embedding(audio_file):\n    # Load the audio file\n    MAX_SIZE= 10000\n    y, sr = librosa.load(audio_file)\n    y = y[:MAX_SIZE]\n    mfccs = librosa.feature.mfcc(y=y, sr=44000, n_mfcc=1)\n    mfccs =  mfccs.squeeze().tolist()\n    return mfccs\n\nif not get_chunking_datatype:\n    e =  vector(shape=(1000,))\nelse:\n    e = get_chunking_datatype(1000)\n\nmodel= ObjectModel(identifier='my-model-audio', object=audio_embedding, datatype=e)        \n"})})})]}),"\n",(0,r.jsx)(l.A,{filename:"build_multimodal_embedding_models.md"})]})}function b(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},94:(e,t,n)=>{n.d(t,{A:()=>s});var r=n(6540);const o=e=>{const t=Array.from(document.querySelectorAll('.tabs > li[role="tab"]')).filter((e=>"true"===e.getAttribute("aria-selected"))).map((e=>e.textContent.trim()));console.log("About to process filename:",e),console.log("Selected tabs:",t);const n=`_${e.replace(/\.md$/,".ipynb")}`,r=encodeURIComponent(n);fetch(`https://build-use-cases-sddb.replit.app/build_notebook?usecase_path=.%2Fuse_cases%2F${r}`,{method:"POST",headers:{"Content-Type":"application/json",Accept:"application/json"},body:JSON.stringify(t)}).then((e=>e.blob())).then((e=>{const t=window.URL.createObjectURL(e),r=document.createElement("a");r.style.display="none",r.href=t,r.download=n,document.body.appendChild(r),r.click(),window.URL.revokeObjectURL(t),alert("Your file has downloaded!")})).catch((()=>alert("There was an error."))),console.log("Sending JSON payload:",JSON.stringify(t))};var a=n(4848);const s=e=>{let{filename:t}=e;if(!t)return console.error("Filename is not provided or invalid."),null;const[n,s]=(0,r.useState)(!1),l={padding:"10px",borderRadius:"10px",border:"0",color:"#000",backgroundColor:"#C4F800",fontWeight:"bold",cursor:"pointer"};return(0,a.jsx)("button",{style:n?{...l,backgroundColor:"#B0E000"}:l,onMouseEnter:()=>s(!0),onMouseLeave:()=>s(!1),onClick:()=>o(t),children:"Generate notebook from all selected tabs"})}},7227:(e,t,n)=>{n.d(t,{A:()=>s});n(6540);var r=n(870);const o={tabItem:"tabItem_Ymn6"};var a=n(4848);function s(e){let{children:t,hidden:n,className:s}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,s),hidden:n,children:t})}},9489:(e,t,n)=>{n.d(t,{A:()=>j});var r=n(6540),o=n(870),a=n(4245),s=n(6347),l=n(6494),i=n(2814),d=n(5167),u=n(1269);function c(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??function(e){return c(e).map((e=>{let{props:{value:t,label:n,attributes:r,default:o}}=e;return{value:t,label:n,attributes:r,default:o}}))}(n);return function(e){const t=(0,d.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function p(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function b(e){let{queryString:t=!1,groupId:n}=e;const o=(0,s.W6)(),a=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,i.aZ)(a),(0,r.useCallback)((e=>{if(!a)return;const t=new URLSearchParams(o.location.search);t.set(a,e),o.replace({...o.location,search:t.toString()})}),[a,o])]}function h(e){const{defaultValue:t,queryString:n=!1,groupId:o}=e,a=m(e),[s,i]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!p({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=n.find((e=>e.default))??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:a}))),[d,c]=b({queryString:n,groupId:o}),[h,f]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[o,a]=(0,u.Dv)(n);return[o,(0,r.useCallback)((e=>{n&&a.set(e)}),[n,a])]}({groupId:o}),g=(()=>{const e=d??h;return p({value:e,tabValues:a})?e:null})();(0,l.A)((()=>{g&&i(g)}),[g]);return{selectedValue:s,selectValue:(0,r.useCallback)((e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),f(e)}),[c,f,a]),tabValues:a}}var f=n(1062);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=n(4848);function _(e){let{className:t,block:n,selectedValue:r,selectValue:s,tabValues:l}=e;const i=[],{blockElementScrollPositionUntilNextRender:d}=(0,a.a_)(),u=e=>{const t=e.currentTarget,n=i.indexOf(t),o=l[n].value;o!==r&&(d(t),s(o))},c=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const n=i.indexOf(e.currentTarget)+1;t=i[n]??i[0];break}case"ArrowLeft":{const n=i.indexOf(e.currentTarget)-1;t=i[n]??i[i.length-1];break}}t?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},t),children:l.map((e=>{let{value:t,label:n,attributes:a}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:r===t?0:-1,"aria-selected":r===t,ref:e=>i.push(e),onKeyDown:c,onClick:u,...a,className:(0,o.A)("tabs__item",g.tabItem,a?.className,{"tabs__item--active":r===t}),children:n??t},t)}))})}function v(e){let{lazy:t,children:n,selectedValue:o}=e;const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=a.find((e=>e.props.value===o));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:a.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==o})))})}function y(e){const t=h(e);return(0,x.jsxs)("div",{className:(0,o.A)("tabs-container",g.tabList),children:[(0,x.jsx)(_,{...e,...t}),(0,x.jsx)(v,{...e,...t})]})}function j(e){const t=(0,f.A)();return(0,x.jsx)(y,{...e,children:c(e.children)},String(t))}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>l});var r=n(6540);const o={},a=r.createContext(o);function s(e){const t=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(a.Provider,{value:t},e.children)}}}]);